{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "\n",
    "Linear regression is the simplest model in machine learning, while it is an important part of many complex models, such as neural network.\n",
    "\n",
    "In this section, we will implement a simple linear regression model and optimize its parameters by gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "import uarray as ua\n",
    "import unumpy as np\n",
    "import numpy as onp\n",
    "import udiff\n",
    "from unumpy import numpy_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "def synthetic_data(w, b, num_examples):\n",
    "    \"\"\"Generate y = Xw + b + noise.\"\"\"\n",
    "    X = onp.random.normal(0, 1, (num_examples, len(w)))\n",
    "    y = onp.dot(X, w) + b\n",
    "    y += onp.random.normal(0, 0.01, y.shape)\n",
    "    return np.asarray(X), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "def gradient_descent(params, loss, lr=0.1):\n",
    "    \"\"\"Gradient Descent.\"\"\"\n",
    "    for param in params:\n",
    "        param._value -= lr * loss.to(param).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\nTrue\n"
    }
   ],
   "source": [
    "with ua.set_backend(udiff.DiffArrayBackend(numpy_backend), coerce=True):\n",
    "        # hyper-parameters\n",
    "        lr = 0.1\n",
    "        epoch = 100\n",
    "        num_examples = 1000\n",
    "\n",
    "        # generate dataset\n",
    "        true_w = onp.array([[2], [-3.4]])\n",
    "        true_b = 4.2\n",
    "        features, labels = synthetic_data(true_w, true_b, num_examples)\n",
    "\n",
    "        # trainable parameters\n",
    "        W = np.asarray(onp.random.normal(scale=0.01, size=(2, 1)))\n",
    "        b = np.zeros(1)\n",
    "        params = [W, b]\n",
    "\n",
    "        # define model and loss function\n",
    "        net = lambda X: np.matmul(X, W) + b\n",
    "        # mean squared error\n",
    "        loss = lambda y_hat, y: np.sum((y_hat - y) ** 2) / num_examples\n",
    "\n",
    "        # train\n",
    "        for e in range(epoch):\n",
    "            y_hat = net(features)\n",
    "            l = loss(y_hat, labels)\n",
    "            gradient_descent(params, l, lr=lr)\n",
    "\n",
    "        print(onp.allclose(W.value, true_w, 0.1))\n",
    "        print(onp.allclose(b.value, true_b, 0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}